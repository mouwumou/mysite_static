<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data: 'self';img-src 'self' https://* data:;media-src 'self';style-src 'self' 'unsafe-inline';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://website.jingxiqiu.com name=base><title>
仇镜溪的博客 • LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models</title><link title="仇镜溪的博客 - Atom Feed" href=https://website.jingxiqiu.com/atom.xml rel=alternate type=application/atom+xml><link href="https://website.jingxiqiu.com/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://website.jingxiqiu.com/main.css?h=fe8c9607d92b0cda0f8f" rel=stylesheet><link href="https://website.jingxiqiu.com/css/override.css?h=207ace6c0589d9a9e471" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models" name=description><meta content="LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models" property=og:description><meta content="LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models" property=og:title><meta content=article property=og:type><meta content=zh_CN property=og:locale:alternate><link href=https://website.jingxiqiu.com/zh/blog/scaling-law/ hreflang=zh rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://website.jingxiqiu.com/blog/scaling-law/ hreflang=en rel=alternate><meta content=https://website.jingxiqiu.com/zh/blog/scaling-law/ property=og:url><meta content=仇镜溪的博客 property=og:site_name><noscript><link href=https://website.jingxiqiu.com/no_js.css rel=stylesheet></noscript><script src=https://website.jingxiqiu.com/js/initializeTheme.min.js></script><script defer src=https://website.jingxiqiu.com/js/themeSwitcher.min.js></script><script src="https://website.jingxiqiu.com/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://website.jingxiqiu.com/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://website.jingxiqiu.com/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://website.jingxiqiu.com/zh/>仇镜溪的博客</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/>首页 </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/blog/>博客 </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/archive/>归档 </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/tags/>标签 </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/projects/>项目 </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/zh/about/>关于 </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="点击或按 $SHORTCUT 开启搜索" class="search-icon interactive-icon" title="点击或按 $SHORTCUT 开启搜索" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class=language-switcher><details class=dropdown><summary aria-haspopup=true aria-label=语言选择 role=button title=语言选择><div class=language-switcher-icon></div></summary> <div class=dropdown-content role=menu>简体中文<a aria-label=English href=https://website.jingxiqiu.com/blog/scaling-law/ lang=en role=menuitem>English</a></div></details><li class="theme-switcher-wrapper js"><div aria-label=切换到暗模式 aria-pressed=false class=theme-switcher role=button tabindex=0 title=切换到暗/亮模式></div><div class="theme-resetter arrow" aria-hidden=true aria-label=将模式重置为网站默认值 role=button tabindex=0 title=将模式重置为网站默认值></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models</h1><a class="u-url u-uid" href=https://website.jingxiqiu.com/zh/blog/scaling-law/></a><ul class=meta><span class="hidden p-author h-card"> <a title="Jingxi Qiu" class=u-url href=https://website.jingxiqiu.com rel=author>Jingxi Qiu</a> </span><li><time class=dt-published datetime=2024-03-20>20 3月 2024</time><li title="2482 字"><span aria-hidden=true class=separator>•</span>13 分钟阅读<li class=tag><span aria-hidden=true class=separator>•</span>标签: <li class=tag><a class=p-category href=https://website.jingxiqiu.com/zh/tags/llm/>LLM</a>, <li class=tag><a class=p-category href=https://website.jingxiqiu.com/zh/tags/scaling-laws/>Scaling Laws</a>, <li class=tag><a class=p-category href=https://website.jingxiqiu.com/zh/tags/lun-wen-zhai-lu/>论文摘录</a></ul><p class=p-summary hidden>LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models<section class="e-content body"><h1 id=llm-scaling-laws-lun-wen-zhai-lu-scaling-laws-for-neural-language-models>LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models</h1><p>type: Post status: Published date: 2024/03/20 tags: AI, LLM, NLP, 论文摘录 category: 技术分享<aside>✋ 许多人都认为大模型的性能和模型的结构、大小、训练计算量、数据集大小有关，但是究竟这些因素如何影响大模型的最终性能？了解这些可以帮助更好地决定究竟投入那些资源来训练所需的模型</aside><h1 id=bei-jing-he-nei-rong>背景和内容</h1><p>OpenAI在2020年发布了《Scaling Laws for Neural Language Models》探讨Scaling Laws（缩放法则），在其中探讨了对于基于Transformerde的大模型的Training Loss与模型参数规模N，数据集大小D，计算量C之间的联系。<p>而在2022年四月谷歌DeepMind在文章《Training Compute-Optimal Large Language Models》中重新讨论了Scaling Laws，他们指出当前的大模型都明显缺乏足够的训练，在使用四倍的数据（相较于280B参数Gopher)训练70B的Chinchilla后取得了更好的成绩（SOTA average accuracy of 67.5% on the MMLU benchmark, 7% increasement）。<p>然而，在2023年二月由<a href=https://espadrine.github.io/ rel=noopener target=_blank><em>Thaddée Tyl</em></a>发布的博客《Chinchilla’s Death》（Chinchilla之死)中，指出，只要训练足够长时间，小模型也能超过大模型。<h1 id=lun-wen>论文</h1><h2 id=scaling-laws-for-neural-language-models>Scaling Laws for Neural Language Models</h2><h3 id=zhu-yao-fa-xian>主要发现</h3><ol><li>模型性能随模型大小N,数据大小D和计算量大小C的增加而提高，和模型形状（深度宽度）与自注意力头弱相关<li>在其他因素不受限的情况下，模型大小N、数据大小D、计算量大小C与性能存在幂律关系<li>同时扩大模型大小N和数据大小D可以提升模型性能，但是文章研究认为当模型大小增加8倍时，数据大小只需要增加5倍，这种情况下不会受到性能惩罚<li>由于训练曲线遵从幂律关系，损失与模型大小无关，因此我们可以大致预测后续训练的损失<li>？在迁移到另一个与训练集不同的数据集上时存在一个惩罚（更大的误差），但是这一个惩罚是恒定的，这意味着其他的提升是共通的<li>大模型相对于小模型有更高的样本效率（sample-effcient），达到相同的水平只需要使用更少的训练(Figure 2)和更少的数据(Figure 4)</ol><p><img alt=Untitled src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled.png><p><img alt=Untitled src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled%201.png><ol><li>收敛是低效的，在计算量大小C固定时，可以在模型没完全收敛之前停止训练来获取最佳性能(Figure 3)。论文给出参考关系为$D\sim C^{0.27}$</ol><p><img alt="这张图体现了在相同的计算量大小C的情况下，来自于不同因素究竟有多少贡献。首先是模型大小，其次是数据（通过更大的batch size和减少复用），而Serial Steps（更多的训练次数）帮助并不大" src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled%202.png><p>这张图体现了在相同的计算量大小C的情况下，来自于不同因素究竟有多少贡献。首先是模型大小，其次是数据（通过更大的batch size和减少复用），而Serial Steps（更多的训练次数）帮助并不大<ol><li>训练的理想batch size应该是训练的幂，并通过梯度噪音尺度（gradient noise scale）继续确定</ol><h3 id=scaling-law-zong-jie>Scaling Law 总结</h3><p><strong>参数：</strong><p>non-embedding parameters $N$<p>the dataset size $D$<p>optimally allocated compute budget $C_{min}$<ul><li><strong>参数含义</strong> <ul><li>$L$ – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in some cases we report the loss for specific tokens within the context.<li>$N$ – the number of model parameters, excluding all vocabulary and positional embeddings<li>$C$ $≈ 6NBS$ – an estimate of the total non-embedding training compute, where $B$ is the batch size, and $S$ is the number of training steps (ie parameter updates). We quote numerical values in PF-days, where one $\text{PF-day}= 10^{15} × 24 × 3600 = 8.64 × 10^{19}$ floating point operations.<li>$D$ – the dataset size in tokens<li>$B_{crit}$ – the critical batch size [MKAT18], defined and discussed in Section 5.1. Training at the critical batch size provides a roughly optimal compromise between time and compute efficiency.<li>$C_{min}$ – an estimate of the minimum amount of non-embedding compute to reach a given value of the loss. This is the training compute that would be used if the model were trained at a batch size much less than the critical batch size.<li>$S_{min}$ – an estimate of the minimal number of training steps needed to reach a given value of the loss. This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size.<li>$\alpha_X$ – power-law exponents for the scaling of the loss as $L(X) ∝ 1/X^{α_X}$ where X can be any of $N, D, C, S, B, C^{min}$.</ul></ul><p>在其他两因素不受限的情况下，测试损失可以由以下公式预测：<ol><li>N受限</ol><p>$$ \begin{equation} L(N)=(N_c/N)^{\alpha_N} \end{equation}<br> $$<p>$$ \alpha_N \sim 0.076,N_c \sim 8.8 \times 10^{13} \text{(non-embedding parameters)} $$<ol><li>D受限（with early stopping)</ol><p>$$ \begin{equation} L(D)=(D_c/D)^{\alpha_D} \end{equation} $$<p>$$ \alpha_D \sim 0.095, D_c \sim 5.4 \times 10^{13}\text{(tokens)} $$<ol><li>C受限</ol><p>$$ \begin{equation}L(C_{min})=(C^{min}<em>c/C</em>{min})^{\alpha_{min}} \end{equation} $$<p>$$ \alpha_C^{min} \sim 0.050, C^{min}_c \sim 3.1 \times 10^8 \text{(PF-days)} $$<p><strong>公式含义：</strong><p>在以上三个公式中,$\alpha_N, \alpha_D, \alpha_C^{min}$给出了当我们提升$N,D,C_{min}$时性能提升的幂次。<p>举个例子，当我们将模型的参数量提升到两倍时，模型的损失将会减小，$2_{-\alpha_N}\approx 0.95$，因此损失将会是此前的0.95倍。而$N_C,D_C,C_C^{min}$的准确数字基于字典大小和tokenization因此没有实际意义，只代表数量级关系。<p>此外，文章还提到了batch size和loss的关系<p>$$ \begin{equation}B_{crit}(L)=\frac{B_*}{L^{1/a_B}} \end{equation} $$<p>$$ B_* \sim 2\cdot10^8 \text{tokens}, \alpha_B\sim0.21 $$<p>根据此前公式(1)和公式(2)可以得出，当我们提升模型大小时，我们应相应地增加数据集的数量，可以根据计算得出$D\propto N^{\frac{\alpha_{N}}{\alpha_D}} \sim N^{0.74}$。他们还发现一个结合(1)和(2)的公式来控制N和D的依赖以及控制过拟合：<p>$$ \begin{equation}L(N,D)=\left[\left(\frac{N_c}{N}^{\frac{\alpha_N}{\alpha_D}}+\frac{D_c}{D} \right) \right]^{\alpha_D}\end{equation} $$<ul><li>作者推测这个函数也能生成其他生成式任务的最大对数似然</ul><p>训练的曲线也可以由训练step数得出，因此可以求得最佳训练step数<p>$$ \begin{equation}L(N,S)=\left(\frac{N_c}{N}\right)^{\alpha{N}}+\left(\frac{S_c}{S_{min}(S)}\right)^{\alpha_S}\end{equation} $$<p>$S_c \approx 2.1 \times 10^3,\alpha_S \approx 0.76$ $S_{min}(S)$ is the minimum possible number of optimization steps (parameter updates) estimated using Equation<p>在固定计算量C的情况下，又得出了以下关系公式<p>$$ \begin{equation}N \propto C^{\alpha^{min}_C /\alpha_N}, B \propto C^{\alpha^{min}_C /\alpha_B}, S \propto C^{\alpha^{min}_C /\alpha_S}, D = B \cdot S\end{equation} $$<p>此处有<p>$$ \begin{equation}\alpha^{min}_C=1/(1/\alpha_S+1/\alpha_B+1/\alpha_N)\end{equation} $$<p>可得$N \propto C^{0.73}<em>{min}, B \propto C^{0.24}</em>{min},\text{ and }S \propto C^{0.03}_{min}$，此处提出观点：<ol><li>当计算量C预算提升的时候，应该主要将其用于更大的模型，而不是更多的训练时间和数据大小。<li>同时当模型变得更大时，他们变得更加sample efficient。</ol><h3 id=yan-jiu-fang-fa>研究方法</h3><p>研究在数据集WebText2及其拓展（$2.29\times 10^{10}$ tokens)，tokenize方法为 byte-pair encoding，词汇大小$n_{vocab}=50257$，性能指标（Loss）为在1024个token上下文的中的交叉熵损失。模型使用的是decoder-only的Transformer，同时训练了LSTM和其他类型的Transformer作为比对。<p>除特别说明外，模型的训练使用了Adam优化器和$2.5 \times 10^5$步，batch size为512，上下文512 token。由于内存限制，最大的模型使用了Adafactor优化器。<p>除特别说明外，训练的学习率是一个3000步的热身和一个cosine decay余弦衰减到零。<p><strong>模型的参数计算方法</strong><p>为了计算模型参数和计算量，模型的超参数定义为：<p>| $n_{layer}$ | 层数 number of layers | | — | — | | $d_{model}$ | 残差流的维度 dimension of the residual stream | | $d_{ff}$ | 前馈层（全连接）的维度 dimension of the intermediate feed-forward layer | | $d_{attn}$ | 注意力输出的维度 dimension of the attention output | | $n_{heads}$ | 每层注意力头数量 number of attention heads per layer | | $n_{ctx}$ | 上下文词元数量，除另说明外为1024 input context |<p>使用$N$代表模型的参数大小，这里定义为除去embedding的参数：<p>$$ \begin{aligned} N&\approx 2d_{model}n_{layer}(2d_{attn}+d_{ff}) \ &= 12n_{layer}d^2_{model}\end{aligned} $$<p>$$ d_{attn}=d_{ff}/4=d_{model} $$<p>这里省去了embedding层的$n_{vocab}d_{model}$和$n_{ctx}d_{model}$参数，向前传递大概需要计算量$C$如下表示：<p>$$ C_{forward} \approx 2N+2n_{layer}n_{ctx}d_{model} $$<p><img alt=Untitled src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled%203.png><h3 id=shi-yan-jie-guo>实验结果</h3><p>实验变量：<ul><li>模型大小（非嵌入参数从768个到15亿个不等）<li>数据集大小（从2200万到230亿个token）<li>形状（包括深度、宽度、注意力头和前馈维度）<li>上下文长度（1024，但也会尝试更短的上下文）<li>批量大小（$2^{19}$，但也会改变它以测量临界批量大小）</ul><p><img alt=Untitled src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled%204.png><p><img alt=Untitled src=https://website.jingxiqiu.com/zh/blog/scaling-law/Untitled%205.png><p>结论：<ol><li>在非嵌入模型大小$N$固定的情况下模型的形状对性能的影响很小，调整很大也只影响几个百分点<li>如果计算嵌入的参数大小，那模型的性能和层数有较大关联（左图），但是如果去掉嵌入层的参数大小，则除了小于两层的模型，不同层数模型的性能基本在同一个趋势上（右图）。<li>在LSTM上同样适用，但是LSTM性能比Transformer差一点<li>幂律定理公式成立：</ol><p>$$ \begin{aligned} L(N) &\approx (N_c/N)^{\alpha_N} \ L(D) &\approx (D_c/D)^{\alpha_D} \ L(C_{min})&\approx (C_{c}^{min}/C_{min})^{\alpha_{min}} \end{aligned} $$<h2 id=training-compute-optimal-large-language-models>Training Compute-Optimal Large Language Models</h2><h1 id=can-kao>参考</h1><p>[1] <a href=https://arxiv.org/pdf/2001.08361.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2001.08361.pdf</a><p>[2] <a href=https://arxiv.org/pdf/2203.15556.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2203.15556.pdf</a><p>[3] <a href=https://espadrine.github.io/blog/posts/chinchilla-s-death.html rel=noopener target=_blank>https://espadrine.github.io/blog/posts/chinchilla-s-death.html</a><p>[4] <a href=https://arxiv.org/pdf/2109.10686.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2109.10686.pdf</a><p>[5] <a href=https://self-supervised.cs.jhu.edu/sp2023/files/17.retrieval-augmentation.pdf rel=noopener target=_blank>https://self-supervised.cs.jhu.edu/sp2023/files/17.retrieval-augmentation.pdf</a></section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label class=button for=toc-toggle id=toc-button title=切换目录><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#llm-scaling-laws-lun-wen-zhai-lu-scaling-laws-for-neural-language-models>LLM Scaling Laws 论文摘录 - Scaling Laws for Neural Language Models</a><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#bei-jing-he-nei-rong>背景和内容</a><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#lun-wen>论文</a> <ul><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#scaling-laws-for-neural-language-models>Scaling Laws for Neural Language Models</a> <ul><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#zhu-yao-fa-xian>主要发现</a><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#scaling-law-zong-jie>Scaling Law 总结</a><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#yan-jiu-fang-fa>研究方法</a><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#shi-yan-jie-guo>实验结果</a></ul><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#training-compute-optimal-large-language-models>Training Compute-Optimal Large Language Models</a></ul><li><a href=https://website.jingxiqiu.com/zh/blog/scaling-law/#can-kao>参考</a></ul></div></div></div><a class=no-hover-padding href=# id=top-button title=返回页面顶部> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://website.jingxiqiu.com/katex.min.css rel=stylesheet><script defer src=https://website.jingxiqiu.com/js/katex.min.js></script><script defer src=https://website.jingxiqiu.com/js/mermaid.min.js></script><span class=hidden id=copy-success> 已复制! </span><span class=hidden id=copy-init> 复制代码到剪贴板 </span><script defer src=https://website.jingxiqiu.com/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://website.jingxiqiu.com/zh/atom.xml rel=noopener target=_blank> <img alt=feed loading=lazy src=https://website.jingxiqiu.com/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> 网站基于 <a href=https://www.getzola.org rel=noopener target=_blank>Zola</a> 和 <a href=https://github.com/welpo/tabi rel=noopener target=_blank>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>搜索</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=搜索… role=combobox spellcheck=false><div class="close-icon interactive-icon" id=clear-search role=button tabindex=0 title=清除搜索><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> 没有找到结果</span><span id=one_results> $NUMBER 个结果</span><span id=many_results> $NUMBER 个结果</span><span id=two_results> $NUMBER 个结果</span><span id=few_results> $NUMBER 个结果</span></div><div id=results role=listbox></div></div></div></div></footer>