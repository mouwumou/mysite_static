<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data: 'self';img-src 'self' https://* data:;media-src 'self';style-src 'self' 'unsafe-inline';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://website.jingxiqiu.com name=base><title>
Jingxi Qiu • LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models</title><link title="Jingxi Qiu - Atom Feed" href=https://website.jingxiqiu.com/atom.xml rel=alternate type=application/atom+xml><link href="https://website.jingxiqiu.com/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://website.jingxiqiu.com/main.css?h=fe8c9607d92b0cda0f8f" rel=stylesheet><link href="https://website.jingxiqiu.com/css/override.css?h=207ace6c0589d9a9e471" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models" name=description><meta content="LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models" property=og:description><meta content="LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models" property=og:title><meta content=article property=og:type><meta content=zh_CN property=og:locale:alternate><link href=https://website.jingxiqiu.com/zh/blog/scaling-law/ hreflang=zh rel=alternate><meta content=en_GB property=og:locale:alternate><link href=https://website.jingxiqiu.com/blog/scaling-law/ hreflang=en rel=alternate><meta content=https://website.jingxiqiu.com/blog/scaling-law/ property=og:url><meta content="Jingxi Qiu" property=og:site_name><noscript><link href=https://website.jingxiqiu.com/no_js.css rel=stylesheet></noscript><script src=https://website.jingxiqiu.com/js/initializeTheme.min.js></script><script defer src=https://website.jingxiqiu.com/js/themeSwitcher.min.js></script><script src="https://website.jingxiqiu.com/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://website.jingxiqiu.com>Jingxi Qiu</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/>Home </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/blog/>Blog </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/archive/>Archive </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/tags/>Tags </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/projects/>Projects </a><li><a class="nav-links no-hover-padding" href=https://website.jingxiqiu.com/about/>About </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class=language-switcher><details class=dropdown><summary aria-label="Language selection" title="Language selection" aria-haspopup=true role=button><div class=language-switcher-icon></div></summary> <div class=dropdown-content role=menu>English<a aria-label=简体中文 href=https://website.jingxiqiu.com/zh/blog/scaling-law/ lang=zh role=menuitem>简体中文</a></div></details><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models</h1><a class="u-url u-uid" href=https://website.jingxiqiu.com/blog/scaling-law/></a><ul class=meta><span class="hidden p-author h-card"> <a title="Jingxi Qiu" class=u-url href=https://website.jingxiqiu.com rel=author>Jingxi Qiu</a> </span><li><time class=dt-published datetime=2024-03-20>20th Mar 2024</time><li title="1916 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://website.jingxiqiu.com/tags/llm/>LLM</a>, <li class=tag><a class=p-category href=https://website.jingxiqiu.com/tags/scaling-laws/>Scaling Laws</a>, <li class=tag><a class=p-category href=https://website.jingxiqiu.com/tags/paper-excerpt/>Paper Excerpt</a></ul><p class=p-summary hidden>LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models<section class="e-content body"><h1 id=llm-scaling-laws-paper-excerpt-scaling-laws-for-neural-language-models>LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models</h1><p>type: Post status: Published date: 2024/03/20 tags: AI, LLM, NLP, 论文摘录 category: 技术分享<aside>✋ Many people believe that the performance of large models is related to the model's structure, size, training compute, and dataset size. However, how exactly do these factors influence the final performance of a large model? Understanding this can help in better deciding where to invest resources to train the required models.</aside><h1 id=background-and-content>Background and Content</h1><p>In 2020, OpenAI released the paper “Scaling Laws for Neural Language Models,” exploring Scaling Laws. This paper discusses the relationship between the training loss of large models based on Transformers and the model parameter size (N), dataset size (D), and computational volume (C).<p>In April 2022, Google DeepMind revisited Scaling Laws in their article “Training Compute-Optimal Large Language Models.” They pointed out that current large models are significantly under-trained. By using four times the data (compared to the 280B parameter Gopher) to train the 70B parameter Chinchilla, they achieved better results (SOTA average accuracy of 67.5% on the MMLU benchmark, a 7% increase).<p>However, in February 2023, a blog titled “Chinchilla’s Death” by <a href=https://espadrine.github.io/ rel=noopener target=_blank><em>Thaddée Tyl</em></a> argued that with sufficient training time, small models can outperform large models.<h1 id=paper>Paper</h1><h2 id=scaling-laws-for-neural-language-models>Scaling Laws for Neural Language Models</h2><h3 id=key-findings>Key Findings</h3><ol><li>Model performance improves with the increase in model size (N), dataset size (D), and compute (C), and is weakly correlated with the model’s shape (depth and width) and the number of self-attention heads.<li>When other factors are not limited, there is a power-law relationship between model size (N), dataset size (D), compute (C), and performance.<li>Expanding both model size (N) and dataset size (D) simultaneously can improve model performance. However, the study suggests that when the model size increases eightfold, the dataset size only needs to increase fivefold, and this will not incur a performance penalty.<li>Because the training curve follows a power-law relationship, the loss is independent of model size, allowing us to roughly predict the loss for subsequent training.<li>There is a penalty (greater error) when transferring to a dataset different from the training set, but this penalty is constant, meaning other improvements are consistent.<li>Large models have higher sample efficiency compared to smaller models, requiring less training (Figure 2) and less data (Figure 4) to achieve the same level of performance.</ol><p><img alt=Untitled src=https://website.jingxiqiu.com/blog/scaling-law/Untitled.png><p><img alt=Untitled src=https://website.jingxiqiu.com/blog/scaling-law/Untitled%201.png><ol><li>Convergence is inefficient. When the compute (C) is fixed, it is possible to stop training before the model fully converges to achieve optimal performance (Figure 3). The paper provides a reference relationship as follows: $D\sim C^{0.27}$</ol><p><img alt="This figure illustrates the contributions of different factors under the same compute budget (C). Firstly, model size contributes the most, followed by data (achieved through larger batch sizes and reduced data reuse), while increasing the number of serial steps (more training iterations) does not significantly help." src=https://website.jingxiqiu.com/blog/scaling-law/Untitled%202.png><p>This figure illustrates the contributions of different factors under the same compute budget (C). Firstly, model size contributes the most, followed by data (achieved through larger batch sizes and reduced data reuse), while increasing the number of serial steps (more training iterations) does not significantly help.<ol><li>The ideal batch size for training should be proportional to the power of the training data size and can be further determined using the gradient noise scale.</ol><h3 id=scaling-law-summary>Scaling Law Summary</h3><p><strong>Parameters：</strong><p>non-embedding parameters $N$<p>the dataset size $D$<p>optimally allocated compute budget $C_{min}$<ul><li><strong>Parameter Definition</strong> <ul><li>$L$ – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in some cases we report the loss for specific tokens within the context.<li>$N$ – the number of model parameters, excluding all vocabulary and positional embeddings<li>$C$ $≈ 6NBS$ – an estimate of the total non-embedding training compute, where $B$ is the batch size, and $S$ is the number of training steps (ie parameter updates). We quote numerical values in PF-days, where one $\text{PF-day}= 10^{15} × 24 × 3600 = 8.64 × 10^{19}$ floating point operations.<li>$D$ – the dataset size in tokens<li>$B_{crit}$ – the critical batch size [MKAT18], defined and discussed in Section 5.1. Training at the critical batch size provides a roughly optimal compromise between time and compute efficiency.<li>$C_{min}$ – an estimate of the minimum amount of non-embedding compute to reach a given value of the loss. This is the training compute that would be used if the model were trained at a batch size much less than the critical batch size.<li>$S_{min}$ – an estimate of the minimal number of training steps needed to reach a given value of the loss. This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size.<li>$\alpha_X$ – power-law exponents for the scaling of the loss as $L(X) ∝ 1/X^{α_X}$ where X can be any of $N, D, C, S, B, C^{min}$.</ul></ul><p>When the other two factors are not limited, the test loss can be predicted using the following formula:<ol><li>N is limited</ol><p>$$ \begin{equation} L(N)=(N_c/N)^{\alpha_N} \end{equation}<br> $$<p>$$ \alpha_N \sim 0.076,N_c \sim 8.8 \times 10^{13} \text{(non-embedding parameters)} $$<ol><li>D is limited（with early stopping)</ol><p>$$ \begin{equation} L(D)=(D_c/D)^{\alpha_D} \end{equation} $$<p>$$ \alpha_D \sim 0.095, D_c \sim 5.4 \times 10^{13}\text{(tokens)} $$<ol><li>C is limited</ol><p>$$ \begin{equation}L(C_{min})=(C^{min}<em>c/C</em>{min})^{\alpha_{min}} \end{equation} $$<p>$$ \alpha_C^{min} \sim 0.050, C^{min}_c \sim 3.1 \times 10^8 \text{(PF-days)} $$<p><strong>Formula Meaning:</strong><p>In the above formulas, $\alpha_N, \alpha_D, \alpha_C^{min}$ indicate the power law exponents that describe the performance improvement when increasing $N,D,C_{min}$.<p>For example, if we double the number of model parameters, the model loss will decrease by a factor of $2_{-\alpha_N}\approx 0.95$, meaning the loss will be 0.95 times the previous value. The exact numbers for $N_C,D_C,C_C^{min}$ are based on vocabulary size and tokenization, thus only representing an order of magnitude relationship rather than precise values.<p>Additionally, the paper discusses the relationship between batch size and loss.<p>$$ \begin{equation}B_{crit}(L)=\frac{B_*}{L^{1/a_B}} \end{equation} $$<p>$$ B_* \sim 2\cdot10^8 \text{tokens}, \alpha_B\sim0.21 $$<p>Based on the previous formulas (1) and (2), when we increase the model size, we should correspondingly increase the dataset size. It can be calculated as:$D\propto N^{\frac{\alpha_{N}}{\alpha_D}} \sim N^{0.74}$。<p>They also derived a combined formula from (1) and (2) to manage the dependencies of N and D and to control overfitting:<p>$$ \begin{equation}L(N,D)=\left[\left(\frac{N_c}{N}^{\frac{\alpha_N}{\alpha_D}}+\frac{D_c}{D} \right) \right]^{\alpha_D}\end{equation} $$<ul><li>The authors speculate that this function can also generate the maximum log-likelihood for other generative tasks.</ul><p>The training curve can also be derived from the number of training steps, allowing us to determine the optimal number of training steps.<p>$$ \begin{equation}L(N,S)=\left(\frac{N_c}{N}\right)^{\alpha{N}}+\left(\frac{S_c}{S_{min}(S)}\right)^{\alpha_S}\end{equation} $$<p>$S_c \approx 2.1 \times 10^3,\alpha_S \approx 0.76$ $S_{min}(S)$ is the minimum possible number of optimization steps (parameter updates) estimated using Equation<p>In the case of fixed compute C, the following relationship formula is derived:<p>$$ \begin{equation}N \propto C^{\alpha^{min}_C /\alpha_N}, B \propto C^{\alpha^{min}_C /\alpha_B}, S \propto C^{\alpha^{min}_C /\alpha_S}, D = B \cdot S\end{equation} $$<p>Here we have:<p>$$ \begin{equation}\alpha^{min}_C=1/(1/\alpha_S+1/\alpha_B+1/\alpha_N)\end{equation} $$<p>So we can get $N \propto C^{0.73}<em>{min}, B \propto C^{0.24}</em>{min},\text{ and }S \propto C^{0.03}_{min}$，here drop out the ideas：<ol><li><p>When the compute budget C is increased, it should primarily be used to create larger models rather than extending training time or increasing dataset size.</p><li><p>Additionally, as models become larger, they become more sample efficient.</p></ol><h3 id=research-methodology>Research Methodology</h3><p>The study was conducted using the WebText2 dataset and its extension (2.29 × 10^10 tokens). The tokenization method employed was byte-pair encoding, with a vocabulary size of $n_{vocab}=50257$. The performance metric (Loss) was the cross-entropy loss over a context of 1024 tokens. The primary model used was a decoder-only Transformer, and LSTM along with other types of Transformers were also trained for comparison.<p>Unless otherwise specified, the model training utilized the Adam optimizer for $2.5 \times 10^5$ steps with a batch size of 512 and a context of 512 tokens. Due to memory constraints, the largest models were trained using the Adafactor optimizer.<p>The learning rate schedule, unless otherwise noted, included a warm-up period of 3000 steps followed by cosine decay to zero.<p><strong>Model Parameter Calculation Method</strong><p>To calculate the model parameters and compute requirements, the model hyperparameters are defined as follows:<table><thead><tr><th>Symbol<th>Description<tbody><tr><td>$n_{layer}$<td>Number of layers<tr><td>$d_{model}$<td>Dimension of the residual stream<tr><td>$d_{ff}$<td>Dimension of the intermediate feed-forward layer<tr><td>$d_{attn}$<td>Dimension of the attention output<tr><td>$n_{heads}$<td>Number of attention heads per layer<tr><td>$n_{ctx}$<td>Number of input context tokens (typically 1024)</table><p>$N$ to represent the size of the model parameters, excluding the embedding layers, the calculation for model parameters is as follows:<p>$$ \begin{aligned} N&\approx 2d_{model}n_{layer}(2d_{attn}+d_{ff}) \ &= 12n_{layer}d^2_{model}\end{aligned} $$<p>$$ d_{attn}=d_{ff}/4=d_{model} $$<p>Here, the parameters for the embedding layers $n_{vocab}d_{model}$ and $n_{ctx}d_{model}$ are omitted.<p>The compute required for a forward pass, denoted as $C$, is approximately:<p>$$ C_{forward} \approx 2N+2n_{layer}n_{ctx}d_{model} $$<p><img alt=Untitled src=https://website.jingxiqiu.com/blog/scaling-law/Untitled%203.png><h3 id=experimental-results>Experimental Results</h3><p><strong>Experimental Variables:</strong><ul><li><strong>Model Size:</strong> Ranging from 768 non-embedding parameters to 1.5 billion parameters.<li><strong>Dataset Size:</strong> From 22 million to 23 billion tokens.<li><strong>Model Shape:</strong> Including variations in depth, width, attention heads, and feed-forward dimensions.<li><strong>Context Length:</strong> Typically 1024 tokens, but shorter contexts were also tested.<li><strong>Batch Size:</strong> Typically $2^{19}$, but varied to measure the critical batch size.</ul><p><img alt=Untitled src=https://website.jingxiqiu.com/blog/scaling-law/Untitled%204.png><p><img alt=Untitled src=https://website.jingxiqiu.com/blog/scaling-law/Untitled%205.png><p><strong>Conclusion:</strong><ol><li>When the non-embedding model size $N$ is fixed, the model shape has a minimal impact on performance, with large adjustments affecting performance by only a few percentage points.<li>If the embedding parameters are included, model performance shows a significant correlation with the number of layers (left graph). However, if the embedding parameters are excluded, the performance of models with different numbers of layers follows the same trend, except for models with fewer than two layers (right graph).<li>The same applies to LSTM models, although LSTM performance is slightly inferior to Transformers.<li>The power law theorem formula holds:</ol><p>$$ \begin{aligned} L(N) &\approx (N_c/N)^{\alpha_N} \ L(D) &\approx (D_c/D)^{\alpha_D} \ L(C_{min})&\approx (C_{c}^{min}/C_{min})^{\alpha_{min}} \end{aligned} $$<h1 id=can-kao>参考</h1><p>[1] <a href=https://arxiv.org/pdf/2001.08361.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2001.08361.pdf</a><p>[2] <a href=https://arxiv.org/pdf/2203.15556.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2203.15556.pdf</a><p>[3] <a href=https://espadrine.github.io/blog/posts/chinchilla-s-death.html rel=noopener target=_blank>https://espadrine.github.io/blog/posts/chinchilla-s-death.html</a><p>[4] <a href=https://arxiv.org/pdf/2109.10686.pdf rel=noopener target=_blank>https://arxiv.org/pdf/2109.10686.pdf</a><p>[5] <a href=https://self-supervised.cs.jhu.edu/sp2023/files/17.retrieval-augmentation.pdf rel=noopener target=_blank>https://self-supervised.cs.jhu.edu/sp2023/files/17.retrieval-augmentation.pdf</a></section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#llm-scaling-laws-paper-excerpt-scaling-laws-for-neural-language-models>LLM Scaling Laws Paper Excerpt - Scaling Laws for Neural Language Models</a><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#background-and-content>Background and Content</a><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#paper>Paper</a> <ul><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#scaling-laws-for-neural-language-models>Scaling Laws for Neural Language Models</a> <ul><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#key-findings>Key Findings</a><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#scaling-law-summary>Scaling Law Summary</a><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#research-methodology>Research Methodology</a><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#experimental-results>Experimental Results</a></ul></ul><li><a href=https://website.jingxiqiu.com/blog/scaling-law/#can-kao>参考</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://website.jingxiqiu.com/katex.min.css rel=stylesheet><script defer src=https://website.jingxiqiu.com/js/katex.min.js></script><script defer src=https://website.jingxiqiu.com/js/mermaid.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://website.jingxiqiu.com/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://website.jingxiqiu.com/atom.xml rel=noopener target=_blank> <img alt=feed loading=lazy src=https://website.jingxiqiu.com/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org rel=noopener target=_blank>Zola</a> & <a href=https://github.com/welpo/tabi rel=noopener target=_blank>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>